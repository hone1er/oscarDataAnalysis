{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\Anaconda3\\lib\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import sklearn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the nominees\n",
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        result: dict, movie name as key, movie information as value\n",
    "    \"\"\"\n",
    "    # remember to use browser header here, or cannot retrieve full data from the website\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "    result = {}\n",
    "    for item in soup.find_all('div', {'class':'result-subgroup subgroup-awardcategory-chron'}):\n",
    "        try:\n",
    "            award_title = item.find('div',{'class':'result-subgroup-title'}).find('a',{'class':'nominations-link'}).contents[0]\n",
    "            if award_title == 'BEST PICTURE':\n",
    "                sub_groups = item.find_all('div',{'class':'result-details awards-result-actingorsimilar'})\n",
    "                for sub in sub_groups:\n",
    "                    sub_result = {}\n",
    "                    film_title = sub.find('div',{'class':'awards-result-film-title'}).find('a',{'class':'nominations-link'}).contents[0]\n",
    "                    statement = sub.find('div',{'class':'awards-result-nominationstatement'}).find('a',{'class':'nominations-link'}).contents[0]           \n",
    "                    sub_result['film_title'] = film_title\n",
    "                    sub_result['statement'] = statement\n",
    "                    sub_result['is_winner'] = 1\n",
    "                    if sub.find('span',{'class':'glyphicon glyphicon-star'}):\n",
    "                        sub_result['winner'] = 1\n",
    "                    else:\n",
    "                        sub_result['winner'] = 0\n",
    "                        \n",
    "                    result[film_title] = sub_result\n",
    "                    \n",
    "        except Exception:\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to scrap the data from The Official Academy Awards Database and append it to the current data set.\n",
    "- If you want to know the alignment of the html framework on Oscar website, you can right click on the Chrome webpage, select “Inspect”, then you will enter the Chrome development mode which you can find the position that the information is located.\n",
    "- The value in “nominee” will be 1 for all movies since the movies retrieved from Oscar website are all nominees.\n",
    "- The value in \"winner\" will be 1 for all winners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data of year 2016\n",
    "best_2016 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:89,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')\n",
    "\n",
    "# data of year 2017\n",
    "best_2017 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:90,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')\n",
    "\n",
    "# data of year 2018\n",
    "best_2018 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:91,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The output should be a json like below \n",
    "{'Black Panther': \n",
    "  {'film_title': 'Black Panther',\n",
    "  'statement': 'Kevin Feige, Producer',\n",
    "  'nominee': 1,\n",
    "  'winner': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "#Movies data set contains all movies by years + 2018 nominatted  movies\n",
    "# credits contains the detailed information of the movies like cast,crew \n",
    "#for 2016,2017 and 2018 since there is no data in kaggle we use tmdb api to scrape the movies info\n",
    "#awards dataset contain all the nominees by category list for the year till 2015, we will append 2016,17 and 2018 next by appending tmdb scarped data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# read from award\n",
    "df_all_awards = pd.read_csv('database.csv', skiprows = 1, names=['year','ceremony','award','is_winner','winner','movieName','filmInfo'])\n",
    "df_all_movies = pd.read_csv('movies.csv')\n",
    "df_all_credits = pd.read_csv('credits.csv')\n",
    "df2018_all_movies = pd.read_csv('2018_best_picture_tmdb.csv')\n",
    "df1617_movies = pd.read_csv('2016_2017_best_picture_tmdb.csv')\n",
    "#df_credits = df_credits.append(df2018_movies)\n",
    "\n",
    "df_all_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the data we scrapped to the df_allawards\n",
    "data = []\n",
    "for k,v in best_2016.items():\n",
    "    row = []\n",
    "    row.append(['2016', 89, 'Best Picture', v['is_winner'],v['winner'], v['film_title'], v['statement']])\n",
    "    data.append(row[0])\n",
    "  \n",
    "for k,v in best_2017.items():\n",
    "    row = []\n",
    "    row.append(['2017', 90, 'Best Picture', v['is_winner'],v['winner'], v['film_title'], v['statement']])\n",
    "    data.append(row[0])\n",
    "\n",
    "for k,v in best_2018.items():\n",
    "   row = []\n",
    "   row.append(['2018', 91, 'Best Picture', v['is_winner'],v['winner'], v['film_title'], v['statement']])\n",
    "   data.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the column name to the columns!!! Or there will be a runtime error\n",
    "data\n",
    "df_all_awards=df_all_awards.append(pd.DataFrame(data,columns=['year','ceremony','award','is_winner','winner','movieName','filmInfo']),ignore_index=True)\n",
    "#df_all_awards = pd.concat([df_all_awards,data])\n",
    "\n",
    "df_all_awards['year']= df_all_awards['year'].astype(int)\n",
    "#df_all_awards['winner']= df_all_awards['winner'].astype(int)\n",
    "df_all_awards.drop_duplicates(subset=['movieName','year','winner','award'], inplace=True, keep='last')\n",
    "df_all_awards.head()\n",
    "#df_all_awards[df_all_awards['movieName']=='Tom Jones ']\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation - Joining the dataframes\n",
    "# We start by saving only the movies we have credits info for and merge on the same movie id\n",
    "\n",
    "if 'title' in df_all_credits.columns: \n",
    "    df_all_credits = df_all_credits.drop('title',axis=1) \n",
    "    \n",
    "df_all_credits_movies = df_all_credits.set_index('movie_id').join(df_all_movies.set_index('id'))\n",
    "#df_all_credits_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert released date to date format so we can use it later for our max, min date\n",
    "df2018_all_movies['release_date'] = pd.to_datetime(df2018_all_movies['release_date'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert released date to date format so we can use it later for our max, min date\n",
    "df1617_movies['release_date'] = pd.to_datetime(df1617_movies['release_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will append our 2016,2017 and 2018 scarped tmdb info on credits like crew,cast to the credits file\n",
    "df_all_credits_movies = df_all_credits_movies.append(df2018_all_movies)\n",
    "df_all_credits_movies = df_all_credits_movies.append(df1617_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_credits_movies['release_date'] = pd.to_datetime(df_all_credits_movies['release_date'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the earliest release date from the dataset\n",
    "min_year = min(df_all_credits_movies['release_date']) #1916-09-04\n",
    "max_year = max(df_all_credits_movies['release_date']) #2017-02-03\n",
    "print(min_year)\n",
    "print(max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_credits_movies['released_year'] = df_all_credits_movies['release_date'].dt.year.fillna(0.0).astype(int)\n",
    "df_all_credits_movies\n",
    "df_all_awards[df_all_awards['movieName']=='Tom Jones ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_credits_movies.to_csv('df_credits_pictures_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select movies that are only in Outstanding Picture or Best Picture awards category and create a new df, df_picture_awards\n",
    "df_all_picture_awards = df_all_awards.loc[df_all_awards['award'].isin(['Outstanding Picture','Best Picture'])]\n",
    "df_all_picture_awards.reset_index(drop = True, inplace = True)\n",
    "#332 Rows\n",
    "#df_all_picture_awards[df_all_picture_awards['movieName']=='My Fair Lady ']\n",
    "#df_all_credits_movies = df_all_credits_movies.drop(['year2'], axis=1)\n",
    "# Get only movies that hae been released\n",
    "df_all_credits_movies = df_all_credits_movies.loc[df_all_credits_movies['status'].isin(['Released'])]\n",
    "df_all_credits_movies\n",
    "df_all_picture_awards[df_all_picture_awards['year']=='2016']\n",
    "print(\"--------------\")\n",
    "print(df_all_picture_awards.columns)\n",
    "print(\"--------------\")\n",
    "print(df_all_credits_movies.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the null value in ‘nominee’ column with 1\n",
    "# df_picture_awards['winner'].isnull().sum()\n",
    "df_all_picture_awards['is_winner'].fillna(0,inplace = True)\n",
    "#df_all_picture_awards[df_all_picture_awards['winner']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df_all_credits_movies and df_all_picture_awards by movie name\n",
    "\n",
    "# Change the title in the movie to lower case\n",
    "import re\n",
    "movieName = [re.sub(r'[^\\w\\s]','',x) for x in df_all_picture_awards['movieName'].str.lower().str.strip().values]\n",
    "title = [re.sub(r'[^\\w\\s]','',x) for x in df_all_credits_movies['title'].str.lower().str.strip().values]\n",
    "\n",
    "df_all_picture_awards.loc[:,'movie_title'] = movieName \n",
    "df_all_credits_movies.loc[:,'movie_title'] = title\n",
    "\n",
    "# check the dataframe, we can see the string in movie_title now is valid\n",
    "#df_all_picture_awards.movie_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two tables, merge the data\n",
    "df = df_all_picture_awards.merge(df_all_credits_movies, left_on='movie_title', right_on='movie_title', how='right')\n",
    "df = df.reset_index(drop=True)\n",
    "df\n",
    "#df[df['nominee']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns from award table\n",
    "df.drop('homepage',axis = 1, inplace = True)\n",
    "df.drop('year',axis = 1, inplace = True)\n",
    "df.drop('award',axis = 1, inplace = True)\n",
    "df.drop('movieName',axis = 1, inplace = True)\n",
    "df.drop('filmInfo',axis = 1, inplace = True)\n",
    "df.drop('ceremony',axis = 1, inplace = True)\n",
    "df['winner'].fillna(0,inplace=True) #fill NA with 0\n",
    "\n",
    "# Print master df to csv\n",
    "# 4795 rows of movies with appended Best Picture data, that we had crew info for\n",
    "df.to_csv('masterList_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the under-sampling by sampling from the 0-labeled data. \n",
    "# We need to first get a subset of nominated data, then sample from the non-nominated data, \n",
    "# and finally append the sampled data to the subset of nominated data.\n",
    "sns.countplot(x='is_winner', data = df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of 0 labeled data, and the number of 1 labeled data\n",
    "print(len(df.loc[df['is_winner'] == 0])) #4586\n",
    "print(len(df.loc[df['is_winner'] == 1])) #209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#balance the data\n",
    "df_subset_0 = df.loc[df['is_winner'] == 0]\n",
    "df_subset_1 = df.loc[df['is_winner'] == 1]\n",
    "#df_subset_1['winner']\n",
    "df_subset_0.drop('is_winner',axis=1,inplace=True)\n",
    "df_subset_1.drop('is_winner',axis=1,inplace=True)\n",
    "\n",
    "# #sample with replacement\n",
    "#df_subset_0 = df_subset_0.sample(150) \n",
    "df_subset = pd.concat([df_subset_0,df_subset_1],ignore_index = True)\n",
    "#df_subset[df_subset['year2']== 2017.0]\n",
    "df_new = df_subset.merge(df_all_picture_awards, left_on = 'movie_title', right_on = 'movie_title', how = 'left')\n",
    "df_new = df_new.fillna(0)\n",
    "df_new\n",
    "#df_new[df_new['year2']== 2017.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of null values in the dataset\n",
    "df_new = df_new.drop(['winner_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.rename(columns={\"winner_x\": \"winner\"})\n",
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "g = sns.heatmap(df_new[['budget','popularity','revenue','runtime','vote_average','vote_count']].corr(),cmap='RdYlGn',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Curve\n",
    "# budget\n",
    "budget0 = df_new[df_new['is_winner'] == 0]['budget']\n",
    "budget1 = df_new[df_new['is_winner'] == 1]['budget']\n",
    "\n",
    "g = sns.kdeplot(budget0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(budget1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revenue\n",
    "# density curve\n",
    "revenue0 = df_new[df_new['is_winner'] == 0]['revenue']\n",
    "revenue1 = df_new[df_new['is_winner'] == 1]['revenue']\n",
    "\n",
    "g = sns.kdeplot(revenue0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(revenue1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote count\n",
    "vc0 = df_new[df_new['is_winner'] == 0]['vote_count']\n",
    "vc1 = df_new[df_new['is_winner'] == 1]['vote_count']\n",
    "\n",
    "g = sns.kdeplot(vc0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(vc1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote average\n",
    "va0 = df_new[df_new['is_winner'] == 0]['vote_average']\n",
    "va1 = df_new[df_new['is_winner'] == 1]['vote_average']\n",
    "\n",
    "g = sns.kdeplot(va0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(va1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity\n",
    "popularity0 = df_new[df_new['is_winner'] == 0]['popularity']\n",
    "popularity1 = df_new[df_new['is_winner'] == 1]['popularity']\n",
    "\n",
    "g = sns.kdeplot(popularity0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(popularity1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime\n",
    "runtime0 = df_new[df_new['is_winner'] == 0]['runtime']\n",
    "runtime1 = df_new[df_new['is_winner'] == 1]['runtime']\n",
    "\n",
    "g = sns.kdeplot(runtime0, legend = True, shade=True, color='r', label = 'non-nominated')\n",
    "g = sns.kdeplot(runtime1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genres\n",
    "#clean the data\n",
    "genres_name0 = {}\n",
    "genres_name1 = {}\n",
    "genres_set = set()\n",
    "for i in range(len(df_new)):\n",
    "    genres = eval(df_new.loc[i,'genres'])\n",
    "    for it in genres:\n",
    "        #print(it)\n",
    "        genres_set.add(it['name'])\n",
    "        if df_new.loc[i,'is_winner'] == 0:\n",
    "            if it['name'] not in genres_name0:\n",
    "                genres_name0[it['name']] = 1\n",
    "            else:\n",
    "                genres_name0[it['name']] += 1\n",
    "        if df_new.loc[i,'is_winner'] == 1:\n",
    "            if it['name'] not in genres_name1:\n",
    "                genres_name1[it['name']] = 1\n",
    "            else:\n",
    "                genres_name1[it['name']] += 1\n",
    "\n",
    "genres_array0=[]\n",
    "genres_array1=[]\n",
    "for g in genres_set:\n",
    "    if g in genres_name0:\n",
    "        genres_array0.append(genres_name0[g])\n",
    "    else:\n",
    "        genres_array0.append(0)\n",
    "    if g in genres_name1:\n",
    "        genres_array1.append(genres_name1[g])\n",
    "    else:\n",
    "        genres_array1.append(0)\n",
    "\n",
    "\n",
    "        \n",
    "genres_all = []\n",
    "genres_all.append(np.array(genres_array0)/sum(genres_array0))\n",
    "genres_all.append(np.array(genres_array1)/sum(genres_array1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres = pd.DataFrame(genres_all, columns=list(genres_set))\n",
    "df_genres[[\"Adventure\",\n",
    "\"Fantasy\",\n",
    "\"Animation\",\n",
    "\"War\",\n",
    "\"Music\",\n",
    "\"Documentary\",\n",
    "\"Foreign\",\n",
    "\"Drama\",\n",
    "\"Horror\",\n",
    "\"Family\",\n",
    "\"Crime\",\n",
    "\"Mystery\",\n",
    "\"Science Fiction\",\n",
    "\"History\",\n",
    "\"Action\",\n",
    "\"Comedy\",\n",
    "\"Romance\",\n",
    "\"Western\",\n",
    "\"Thriller\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stacked bar chart\n",
    "N = len(df_genres.columns)\n",
    "ind = np.arange(N)\n",
    "width = 0.5\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "p1 = plt.bar(ind, df_genres.loc[[0]].values[0], width, color='#d62728')\n",
    "p2 = plt.bar(ind, df_genres.loc[[1]].values[0], width, bottom=df_genres.loc[[0]].values[0])\n",
    "\n",
    "plt.ylabel('percentage (#genres/#movies)')\n",
    "plt.title('Percentage by genres and nominations')\n",
    "plt.xticks(ind,df_genres.columns)\n",
    "plt.legend((p1[0],p2[0]),('Non-nominees','nominees'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Since many columns such as “crew”, “cast” contain information in json format, we need to extract useful information from the columns and then perform one hot encoding.\n",
    "# We will transform our dataset into a all numeric matrix so that we can feed the data into our machine learning model.\n",
    "# To look at the structure of column (eg.”cast”), we can use: df.loc[0,’cast’]\n",
    "import json\n",
    "def feature_engineering(column_name, df, json_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        column_name: the column name in the dataframe that contains a json file that needs to conduct feature engineering on\n",
    "        df: dataframe that perform feature engineering on\n",
    "        json_name: name in the json file that we want to extract\n",
    "    \n",
    "    Returns: new dataframe after feature engineering\n",
    "    \"\"\"\n",
    "    \n",
    "    name = {}\n",
    "    fails = []\n",
    "\n",
    "    for item in df[column_name]:\n",
    "        group = eval(item)\n",
    "        #print(type(group))\n",
    "        for it in group:\n",
    "            #print (it)\n",
    "            if it[json_name] not in name:\n",
    "                name[it[json_name]] = 1\n",
    "            else:\n",
    "                name[it[json_name]] += 1\n",
    "    \n",
    "    final = {}\n",
    "    index = 0\n",
    "    for k,v in name.items():\n",
    "        if v > 1:\n",
    "            final[k] = index\n",
    "            index += 1\n",
    "    np_item = np.zeros((len(df),len(final)))\n",
    "    item_dict = {}\n",
    "    row = 0\n",
    "    for item in df[column_name]:\n",
    "        #print(item)\n",
    "        group = eval(item)\n",
    "        for it in group:\n",
    "            if it[json_name] in final:\n",
    "                index = final[it[json_name]]\n",
    "                np_item[row][index] = 1\n",
    "        row += 1\n",
    "\n",
    "    df_item = pd.DataFrame(np_item, columns = list(final.keys()))\n",
    "    df_output = pd.concat([df,df_item],axis = 1)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = feature_engineering(\"cast\", df_new, \"name\")\n",
    "#df2['cast']\n",
    "df2 = df2.drop(columns=['cast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# director\n",
    "crew_name = {}\n",
    "\n",
    "for item in df2['crew']:\n",
    "    crew = eval(item)\n",
    "    for it in crew:\n",
    "        if it['job'] == 'Director':\n",
    "            if it['name'] not in crew_name:\n",
    "                crew_name[it['name']] = 1\n",
    "            else:\n",
    "                crew_name[it['name']]+=1\n",
    "\n",
    "\n",
    "# set the appear tims for actors\n",
    "final_crew = {}\n",
    "index = 0\n",
    "for k,v in crew_name.items():\n",
    "    if v > 0:\n",
    "        final_crew[k] = index\n",
    "        index += 1\n",
    "# print(len(final_crew))\n",
    "\n",
    "np_crew = np.zeros((len(df2), len(final_crew)))\n",
    "row = 0\n",
    "for item in df2['crew']:\n",
    "    crew = eval(item)\n",
    "    for it in crew:\n",
    "        if it['job'] == 'Director':\n",
    "            if it['name'] in final_crew:\n",
    "                index = final_crew[it['name']]\n",
    "                np_crew[row][index] = 1\n",
    "    row += 1\n",
    "\n",
    "df_crew = pd.DataFrame(np_crew, columns = list(final_crew.keys()))\n",
    "            \n",
    "df3 = pd.concat([df2, df_crew], axis = 1)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['crew'],axis=1)\n",
    "# #genres\n",
    "df4 = feature_engineering(\"genres\", df3, \"name\")\n",
    "#df4\n",
    "df4 = df4.drop(['genres'], axis = 1)\n",
    "#keywords\n",
    "df5 = feature_engineering('keywords', df4, 'name')\n",
    "df5 = df5.drop(['keywords'], axis = 1)\n",
    "#production_companies\n",
    "df6 = feature_engineering('production_companies',df5,'name')\n",
    "df6 = df6.drop(['production_companies'],axis=1)\n",
    "#production_countries\n",
    "df7 = feature_engineering('production_countries',df6,'name')\n",
    "df7 = df7.drop(['production_countries'],axis=1)\n",
    "#spoken_languages\n",
    "df8 = feature_engineering('spoken_languages',df7,'iso_639_1')\n",
    "df8 = df8.drop(['spoken_languages'],axis=1)\n",
    "# drop the columns not used\n",
    "df_clean = df8.drop([\"movie_title\",\"original_title\",\"overview\",\"tagline\",'title','original_language','status','release_date','movieName','filmInfo','award'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop_duplicates()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017  = df_clean.loc[df_clean['released_year'] != 2018]\n",
    "df_2018  = df_clean.loc[df_clean['released_year'] == 2018]\n",
    "#df_2018 = df_2018.drop_duplicates(['movie_id'])\n",
    "df_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "# split\n",
    "X_train = df_2017[df_2017.columns.difference(['winner'])]\n",
    "y_train = df_2017['winner']\n",
    "X_test_2018 = df_2018[df_2018.columns.difference(['winner'])]\n",
    "y_test_2018 = df_2018['winner']\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled\n",
    "X_test_scaled = scaler.transform(X_test_2018)\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we choose 0.81 as the parameters in PCA(), which means 0.81 variance in the features will be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.81)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "X_train_pca\n",
    "X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# For small datasets, ‘liblinear’ is a good choice\n",
    "logisticRegr = LogisticRegression(solver = 'liblinear')\n",
    "logisticRegr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation\n",
    "\n",
    "predicted = logisticRegr.predict(X_test_pca)\n",
    "df_2018['prediction'] = predicted\n",
    "print(logisticRegr.predict(X_test_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_test_pca, y_test_2018)\n",
    "# print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2018, predicted).ravel()\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(\"True Positives:\",tp)\n",
    "print(\"False Positives:\",fp)\n",
    "print(\"True Negatives:\",tn)\n",
    "print(\"False Negatives:\",fn)\n",
    "print(sensitivity) # 0.84375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2018 = df_2018[['movie_id','winner','prediction']]\n",
    "#df_p2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = df_p2018.merge(df3, on='movie_id', how='left')\n",
    "df_prediction  = df_prediction.rename(columns={\"winner_x\": \"winner\"})\n",
    "df_prediction[['movie_title','winner','prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_2018, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
