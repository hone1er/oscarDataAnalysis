{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import sklearn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the nominees\n",
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        result: dict, movie name as key, movie information as value\n",
    "    \"\"\"\n",
    "    # remember to use browser header here, or cannot retrieve full data from the website\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #print (\"passing response\")\n",
    "    html = response.content\n",
    "    #print (\"passing html\")\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    #print (\"souping\")\n",
    "\n",
    "    result = {}\n",
    "    for item in soup.find_all('div', {'class':'result-subgroup subgroup-awardcategory-chron'}):\n",
    "        try:\n",
    "            #print('looking for award')\n",
    "            award_title = item.find('div',{'class':'result-subgroup-title'}).find('a',{'class':'nominations-link'}).contents[0]\n",
    "            if award_title == 'BEST PICTURE':\n",
    "                print (\"found best picture\")\n",
    "                sub_groups = item.find_all('div',{'class':'result-details awards-result-actingorsimilar'})\n",
    "                for sub in sub_groups:\n",
    "                    sub_result = {}\n",
    "                    film_title = sub.find('div',{'class':'awards-result-film-title'}).find('a',{'class':'nominations-link'}).contents[0]\n",
    "                    is_winner = 1\n",
    "                    statement = sub.find('div',{'class':'awards-result-nominationstatement'}).find('a',{'class':'nominations-link'}).contents[0]           \n",
    "                    sub_result['film_title'] = film_title\n",
    "                    sub_result['is_winner'] = is_winner\n",
    "                    sub_result['statement'] = statement\n",
    "                    result[film_title] = sub_result\n",
    "        except Exception:\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best picture\n"
     ]
    }
   ],
   "source": [
    "# data of year 2016\n",
    "best_2016 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:89,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')\n",
    "\n",
    "# data of year 2017\n",
    "#best_2017 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:90,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')\n",
    "\n",
    "# data of year 2018\n",
    "#best_2018 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:91,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')\n",
    "\n",
    "# data of year 2019\n",
    "#best_2019 = retrieve_html('http://awardsdatabase.oscars.org/Search/GetResults?query=%7B%22AwardShowFrom%22:92,%22Sort%22:%223-Award%20Category-Chron%22,%22Search%22:%22Basic%22%7D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# read from award\n",
    "df_awards = pd.read_csv('database.csv', skiprows = 1, names=['year','ceremony','award','winner','movieName','filmInfo'])\n",
    "df_movies = pd.read_csv('movies.csv')\n",
    "df_credits = pd.read_csv('credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View data\n",
    "#df_awards #ends at 2015\n",
    "#print(\"\")\n",
    "#print(df_credits.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# append the data we scrapped to the df_awards\n",
    "data = []\n",
    "for k,v in best_2016.items():\n",
    "    row = []\n",
    "    row.append(['2016', 89, 'Best Picture', v['is_winner'], v['film_title'], v['statement']])\n",
    "    data.append(row[0])\n",
    "  \n",
    "for k,v in best_2017.items():\n",
    "    row = []\n",
    "    row.append(['2017', 90, 'Best Picture', v['is_winner'], v['film_title'], v['statement']])\n",
    "    data.append(row[0])\n",
    "\n",
    "#for k,v in best_2018.items():\n",
    "#    row = []\n",
    "#    row.append(['2018', 91, 'Best Picture', v['is_winner'], v['film_title'], v['statement']])\n",
    "#    data.append(row[0])\n",
    "\n",
    "#data #Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pass the column name to the columns!!! Or there will be a runtime error\n",
    "df_awards=df_awards.append(pd.DataFrame(data,columns=['year','ceremony','award','winner','movieName','filmInfo']),ignore_index=True)\n",
    "\n",
    "#df_awards \n",
    "#9982 rows × 6 columns, no duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation - Joining the dataframes\n",
    "# We start by saving only the movies we have credits info for and merge on the same movie id\n",
    "\n",
    "if 'title' in df_credits.columns: \n",
    "  df_credits = df_credits.drop('title',axis=1) \n",
    "df_credits_movies = df_credits.set_index('movie_id').join(df_movies.set_index('id'))\n",
    "#df_credits_movies\n",
    "# 9982 rows --> 4803 rows x 21 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_credits_movies.dtypes\n",
    "\n",
    "# Reset the data types and get the earliest date of release_date\n",
    "df_credits_movies['release_date'] = pd.to_datetime(df_credits_movies['release_date'])\n",
    "#get the earliest release date from the dataset\n",
    "min_year = min(df_credits_movies['release_date']) #1916-09-04\n",
    "max_year = max(df_credits_movies['release_date']) #2017-02-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select movies that are only in Outstanding Picture or Best Picture awards category and create a new df, df_picture_awards\n",
    "df_picture_awards = df_awards.loc[df_awards['award'].isin(['Outstanding Picture','Best Picture'])]\n",
    "df_picture_awards.reset_index(drop = True, inplace = True)\n",
    "#332 Rows\n",
    "\n",
    "# Get only movies that hae been released\n",
    "df_credits_movies = df_credits_movies.loc[df_credits_movies['status'].isin(['Released'])]\n",
    "\n",
    "#print(\"--------------\")\n",
    "#print(df_picture_awards.columns)\n",
    "#print(\"--------------\")\n",
    "#print(df_credits_movies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the null value in ‘winner’ column with 1\n",
    "# df_picture_awards['winner'].isnull().sum()\n",
    "df_picture_awards['winner'].fillna(1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join df_credits_movies and df_picture_awards by movie name\n",
    "\n",
    "# Change the title in the movie to lower case\n",
    "import re\n",
    "movieName = [re.sub(r'[^\\w\\s]','',x) for x in df_picture_awards['movieName'].str.lower().str.strip().values]\n",
    "title = [re.sub(r'[^\\w\\s]','',x) for x in df_credits_movies['title'].str.lower().str.strip().values]\n",
    "\n",
    "df_picture_awards.loc[:,'movie_title'] = movieName \n",
    "df_credits_movies.loc[:,'movie_title'] = title\n",
    "\n",
    "# check the dataframe, we can see the string in movie_title now is valid\n",
    "#df_picture_awards.movie_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two tables, merge the data\n",
    "df = df_picture_awards.merge(df_credits_movies, left_on='movie_title', right_on='movie_title', how='right')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# @Priya - PROBLEM HERE? WE'RE GETTING JUNK DATA AT THE END OF OUR MASTERLIST.CSV, PRETTY SURE IT'S FOR RECENT YEAR'S DATA\n",
    "# Maybe single out the junk data 1st and see if we can clean it up? If you sort the csv by release date you can find the junk.\n",
    "\n",
    "#drop the columns from award table\n",
    "df.drop('homepage',axis = 1, inplace = True)\n",
    "df.drop('year',axis = 1, inplace = True)\n",
    "df.drop('award',axis = 1, inplace = True)\n",
    "df.drop('movieName',axis = 1, inplace = True)\n",
    "df.drop('filmInfo',axis = 1, inplace = True)\n",
    "df.drop('ceremony',axis = 1, inplace = True)\n",
    "df['winner'].fillna(0,inplace=True) #fill NA with 0\n",
    "\n",
    "# Print master df to csv\n",
    "# 4795 rows of movies with appended Best Picture data, that we had crew info for\n",
    "df.to_csv('masterList.csv', index=False)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cast'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the under-sampling by sampling from the 0-labeled data. \n",
    "# We need to first get a subset of nominated data, then sample from the non-nominated data, \n",
    "# and finally append the sampled data to the subset of nominated data.\n",
    "sns.countplot(x='winner', data = df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of 0 labeled data, and the number of 1 labeled data\n",
    "print(len(df.loc[df['winner'] == 0])) #4586\n",
    "print(len(df.loc[df['winner'] == 1])) #209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance the data\n",
    "df_subset_0 = df.loc[df['winner'] == 0]\n",
    "df_subset_1 = df.loc[df['winner'] == 1]\n",
    "\n",
    "df_subset_0.drop('winner',axis=1,inplace=True)\n",
    "df_subset_1.drop('winner',axis=1,inplace=True)\n",
    "\n",
    "#sample with replacement\n",
    "df_subset_0 = df_subset_0.sample(400) \n",
    "df_subset = pd.concat([df_subset_0,df_subset_1],ignore_index = True)\n",
    "df_new = df_subset.merge(df_picture_awards, left_on = 'movie_title', right_on = 'movie_title', how = 'left')\n",
    "df_new = df_new.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='winner', data = df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of null values in the dataset\n",
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis\n",
    "\n",
    "# check the data types and select numeric variables\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "g = sns.heatmap(df_new[['budget','popularity','revenue','runtime','vote_average','vote_count']].corr(),cmap='RdYlGn',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Curve\n",
    "# budget\n",
    "budget0 = df_new[df_new['winner'] == 0]['budget']\n",
    "budget1 = df_new[df_new['winner'] == 1]['budget']\n",
    "\n",
    "g = sns.kdeplot(budget0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(budget1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revenue\n",
    "# density curve\n",
    "revenue0 = df_new[df_new['winner'] == 0]['revenue']\n",
    "revenue1 = df_new[df_new['winner'] == 1]['revenue']\n",
    "\n",
    "g = sns.kdeplot(revenue0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(revenue1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote count\n",
    "vc0 = df_new[df_new['winner'] == 0]['vote_count']\n",
    "vc1 = df_new[df_new['winner'] == 1]['vote_count']\n",
    "\n",
    "g = sns.kdeplot(vc0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(vc1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote average\n",
    "va0 = df_new[df_new['winner'] == 0]['vote_average']\n",
    "va1 = df_new[df_new['winner'] == 1]['vote_average']\n",
    "\n",
    "g = sns.kdeplot(va0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(va1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity\n",
    "popularity0 = df_new[df_new['winner'] == 0]['popularity']\n",
    "popularity1 = df_new[df_new['winner'] == 1]['popularity']\n",
    "\n",
    "g = sns.kdeplot(popularity0, legend = True, shade=True, color='r',label = 'non-nominated')\n",
    "g = sns.kdeplot(popularity1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runtime\n",
    "runtime0 = df_new[df_new['winner'] == 0]['runtime']\n",
    "runtime1 = df_new[df_new['winner'] == 1]['runtime']\n",
    "\n",
    "g = sns.kdeplot(runtime0, legend = True, shade=True, color='r', label = 'non-nominated')\n",
    "g = sns.kdeplot(runtime1, legend = True, shade=True, color='b', label = 'nonminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genres\n",
    "#clean the data\n",
    "genres_name0 = {}\n",
    "genres_name1 = {}\n",
    "genres_set = set()\n",
    "for i in range(len(df_new)):\n",
    "    genres = json.loads(df_new.loc[i,'genres'])\n",
    "    for it in genres:\n",
    "        genres_set.add(it['name'])\n",
    "        if df_new.loc[i,'winner'] == 0:\n",
    "            if it['name'] not in genres_name0:\n",
    "                genres_name0[it['name']] = 1\n",
    "            else:\n",
    "                genres_name0[it['name']] += 1\n",
    "        if df_new.loc[i,'winner'] == 1:\n",
    "            if it['name'] not in genres_name1:\n",
    "                genres_name1[it['name']] = 1\n",
    "            else:\n",
    "                genres_name1[it['name']] += 1\n",
    "\n",
    "genres_array0=[]\n",
    "genres_array1=[]\n",
    "for g in genres_set:\n",
    "    if g in genres_name0:\n",
    "        genres_array0.append(genres_name0[g])\n",
    "    else:\n",
    "        genres_array0.append(0)\n",
    "    if g in genres_name1:\n",
    "        genres_array1.append(genres_name1[g])\n",
    "    else:\n",
    "        genres_array1.append(0)\n",
    "\n",
    "\n",
    "        \n",
    "genres_all = []\n",
    "genres_all.append(np.array(genres_array0)/sum(genres_array0))\n",
    "genres_all.append(np.array(genres_array1)/sum(genres_array1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres = pd.DataFrame(genres_all, columns=list(genres_set))\n",
    "print(df_genres.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw stacked bar chart\n",
    "N = len(df_genres.columns)\n",
    "ind = np.arange(N)\n",
    "width = 0.5\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "p1 = plt.bar(ind, df_genres.loc[[0]].values[0], width, color='#d62728')\n",
    "p2 = plt.bar(ind, df_genres.loc[[1]].values[0], width, bottom=df_genres.loc[[0]].values[0])\n",
    "\n",
    "plt.ylabel('percentage (#genres/#movies)')\n",
    "plt.title('Percentage by genres and nominations')\n",
    "plt.xticks(ind,df_genres.columns)\n",
    "plt.legend((p1[0],p2[0]),('Non-nominees','nominees'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Since many columns such as “crew”, “cast” contain information in json format, we need to extract useful information from the columns and then perform one hot encoding.\n",
    "# We will transform our dataset into a all numeric matrix so that we can feed the data into our machine learning model.\n",
    "# To look at the structure of column (eg.”cast”), we can use: df.loc[0,’cast’]\n",
    "import json\n",
    "def feature_engineering(column_name, df, json_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        column_name: the column name in the dataframe that contains a json file that needs to conduct feature engineering on\n",
    "        df: dataframe that perform feature engineering on\n",
    "        json_name: name in the json file that we want to extract\n",
    "    \n",
    "    Returns: new dataframe after feature engineering\n",
    "    \"\"\"\n",
    "    \n",
    "    name = {}\n",
    "\n",
    "    for item in df[column_name]:\n",
    "        group = json.loads(item)\n",
    "        for it in group:\n",
    "            if it[json_name] not in name:\n",
    "                name[it[json_name]] = 1\n",
    "            else:\n",
    "                name[it[json_name]] += 1\n",
    "    \n",
    "    final = {}\n",
    "    index = 0\n",
    "    for k,v in name.items():\n",
    "        if v > 1:\n",
    "            final[k] = index\n",
    "            index += 1\n",
    "    np_item = np.zeros((len(df),len(final)))\n",
    "    item_dict = {}\n",
    "    row = 0\n",
    "    for item in df[column_name]:\n",
    "        group = json.loads(item)\n",
    "        for it in group:\n",
    "            if it[json_name] in final:\n",
    "                index = final[it[json_name]]\n",
    "                np_item[row][index] = 1\n",
    "        row += 1\n",
    "\n",
    "    df_item = pd.DataFrame(np_item, columns = list(final.keys()))\n",
    "    df_output = pd.concat([df,df_item],axis = 1)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = feature_engineering(\"cast\", df_new, \"name\")\n",
    "df2 = df2.drop('cast', axis = 1) #drop the column after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# director\n",
    "crew_name = {}\n",
    "\n",
    "for item in df2['crew']:\n",
    "    crew = json.loads(item)\n",
    "    for it in crew:\n",
    "        if it['job'] == 'Director':\n",
    "            if it['name'] not in crew_name:\n",
    "                crew_name[it['name']] = 1\n",
    "            else:\n",
    "                crew_name[it['name']]+=1\n",
    "\n",
    "\n",
    "# set the appear tims for actors\n",
    "final_crew = {}\n",
    "index = 0\n",
    "for k,v in crew_name.items():\n",
    "    if v > 0:\n",
    "        final_crew[k] = index\n",
    "        index += 1\n",
    "# print(len(final_crew))\n",
    "\n",
    "np_crew = np.zeros((len(df2), len(final_crew)))\n",
    "row = 0\n",
    "for item in df2['crew']:\n",
    "    crew = json.loads(item)\n",
    "    for it in crew:\n",
    "        if it['job'] == 'Director':\n",
    "            if it['name'] in final_crew:\n",
    "                index = final_crew[it['name']]\n",
    "                np_crew[row][index] = 1\n",
    "    row += 1\n",
    "\n",
    "df_crew = pd.DataFrame(np_crew, columns = list(final_crew.keys()))\n",
    "            \n",
    "df3 = pd.concat([df2, df_crew], axis = 1)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['crew'],axis=1)\n",
    "#genres\n",
    "df4 = feature_engineering(\"genres\", df3, \"name\")\n",
    "df4 = df4.drop(['genres'], axis = 1)\n",
    "#keywords\n",
    "df5 = feature_engineering('keywords', df4, 'name')\n",
    "df5 = df5.drop(['keywords'], axis = 1)\n",
    "#production_companies\n",
    "df6 = feature_engineering('production_companies',df5,'name')\n",
    "df6 = df6.drop(['production_companies'],axis=1)\n",
    "#production_countries\n",
    "df7 = feature_engineering('production_countries',df6,'name')\n",
    "df7 = df7.drop(['production_countries'],axis=1)\n",
    "#spoken_languages\n",
    "df8 = feature_engineering('spoken_languages',df7,'iso_639_1')\n",
    "df8 = df8.drop(['spoken_languages'],axis=1)\n",
    "# drop the columns not used\n",
    "df_clean = df8.drop([\"movie_title\",\"original_title\",\"overview\",\"tagline\",'title','original_language','status','release_date','movieName','filmInfo','award'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "# split\n",
    "X = df_clean[df_clean.columns.difference(['winner'])]\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "y = df_clean['winner']\n",
    "from sklearn.model_selection import train_test_split\n",
    "# test_size: what proportion of original data is used for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=1/7.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.95)\n",
    "fit = pca.fit(X_train)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "# X_test = pca.transform(X_test)\n",
    "# pca = PCA(n_components = 3)\n",
    "# fit = pca.fit_transform(X)\n",
    "# fit2 = pca.fit(X)\n",
    "# print(\"Explained Variance: %s\" % pca.explained_variance_ratio_) \n",
    "# print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# For small datasets, ‘liblinear’ is a good choice\n",
    "logisticRegr = LogisticRegression(solver = 'liblinear')\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for One Observation\n",
    "predicted = logisticRegr.predict(X_test)\n",
    "#print(logisticRegr.predict(X_test))\n",
    "#print(y_test.index)\n",
    "# df8['movie_title'][y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_test, y_test) # 0.8850574712643678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted).ravel()\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(sensitivity) # 0.84375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-fold Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "sumAccuracy = []\n",
    "for train,test in skf.split(X,y):\n",
    "#     logisticRegr.fit(df_clean.iloc[train][], )\n",
    "    df_train = df_clean.iloc[train]\n",
    "    df_test = df_clean.iloc[test]\n",
    "    train_X = df_train[df_clean.columns.difference(['winner'])]\n",
    "    train_y = df_train['winner']\n",
    "    test_X = df_test[df_clean.columns.difference(['winner'])]\n",
    "    test_y = df_test['winner']\n",
    "    logisticRegr.fit(train_X, train_y)\n",
    "    sumAccuracy.append(logisticRegr.score(test_X, test_y))\n",
    "avg = np.mean(sumAccuracy)\n",
    "print(avg) # 0.7751010491916065\n",
    "#     print(df_clean.iloc[train,df_clean.columns.difference(['winner'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test with 2018 Data\n",
    "#best_2018\n",
    "\n",
    "data2 = []\n",
    "for k,v in best_2018.items():\n",
    "    row = []\n",
    "    row.append(['2018', 91, 'Best Picture', v['is_winner'], v['film_title'], v['statement']])\n",
    "    data2.append(row[0]) \n",
    "#data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out we didn't have all the data for 2016-17, printing out a new csv to run through the scraper\n",
    "\n",
    "df_2016_2017=pd.DataFrame(data,columns=['year','ceremony','award','winner','movieName','filmInfo'])\n",
    "\n",
    "#df_2016_2017.to_csv('bestPic2016-2017Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pass the column name to the columns!!! Or there will be a runtime error\n",
    "df_2018=pd.DataFrame(data2,columns=['year','ceremony','award','winner','movieName','filmInfo'])\n",
    "\n",
    "df_2018 \n",
    "#9990 rows × 6 columns, no duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "movieName = [re.sub(r'[^\\w\\s]','',x) for x in df_2018['movieName'].str.lower().str.strip().values]\n",
    "title = [re.sub(r'[^\\w\\s]','',x) for x in df_credits_movies['title'].str.lower().str.strip().values]\n",
    "\n",
    "df_2018.loc[:,'movie_title'] = movieName \n",
    "df_credits_movies.loc[:,'movie_title'] = title\n",
    "\n",
    "df_test = df_2018.merge(df_credits_movies, left_on='movie_title', right_on='movie_title', how='left')\n",
    "\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out 2016, 2017, 2018 data for scraping\n",
    "\n",
    "#df_test.to_csv('bestPic2018Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# @Priya - PROBLEM HERE? WE'RE GETTING JUNK DATA AT THE END OF OUR MASTERLIST.CSV, PRETTY SURE IT'S FOR RECENT YEAR'S DATA\n",
    "# Maybe single out the junk data 1st and see if we can clean it up? If you sort the csv by release date you can find the junk.\n",
    "\n",
    "#drop the columns from award table\n",
    "df_test.drop('homepage',axis = 1, inplace = True)\n",
    "df_test.drop('year',axis = 1, inplace = True)\n",
    "df_test.drop('award',axis = 1, inplace = True)\n",
    "df_test.drop('movieName',axis = 1, inplace = True)\n",
    "df_test.drop('movie_title',axis = 1, inplace = True)\n",
    "df_test.drop('filmInfo',axis = 1, inplace = True)\n",
    "df_test.drop('ceremony',axis = 1, inplace = True)\n",
    "df_test['winner'].fillna(0,inplace=True) #fill NA with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(\"winner\", axis=1)\n",
    "y_test = df_test[\"winner\"]\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "# For small datasets, ‘liblinear’ is a good choice\n",
    "\n",
    "y_pred = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {logisticRegr.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {logisticRegr.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)\n",
    "\n",
    "pd.DataFrame({\"Prediction\": predictions[:20], \"Actual\": y_test[:20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
